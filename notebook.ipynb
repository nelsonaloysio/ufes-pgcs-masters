{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ufes-pgcs\n",
    "---\n",
    "\n",
    "Código desenvolvido e utilizado no âmbito da dissertação produzida para o Curso de Mestrado em Ciências Sociais da Universidade Federal do Espírito Santo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerrequesites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some graph building and analysis functions require [graphkit](https://github.com/nelsonaloysio/graphkit). Stopwords list used: [stopwords.py](https://gist.github.com/nelsonaloysio/302dbbf3963fababde6e9f97669587df) (plus `nltk.corpus.stopwords`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "# import collections\n",
    "# import io\n",
    "# import math\n",
    "# import pkgutil\n",
    "# import re\n",
    "# import string\n",
    "# import urllib\n",
    "from os import listdir\n",
    "# from datetime import datetime\n",
    "# from pprint import pprint\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly.offline as py\n",
    "import scattertext as st\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "# import networkit as nk\n",
    "# import numpy as np\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "from matplotlib.ticker import FuncFormatter, StrMethodFormatter\n",
    "from urllib.request import urlopen\n",
    "# from IPython.core.display import HTML, display\n",
    "# from IPython.display import IFrame\n",
    "# from matplotlib.ticker import IndexFormatter\n",
    "\n",
    "from pygraphkit import GraphKit\n",
    "from notebook_functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up vars and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('mathtext', fontset='stix')\n",
    "matplotlib.rcParams['font.size'] = 11 # 14\n",
    "\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 12}) # 'font.size': 14\n",
    "py.init_notebook_mode = True\n",
    "\n",
    "gk = GraphKit()\n",
    "tokenizer = Tokenizer()\n",
    "plot = Plot(\"ggplot2\")\n",
    "pio.templates.default = 'ggplot2' # 'seaborn'\n",
    "\n",
    "datetime_format = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "params = dict(\n",
    "    # colors='#000000',\n",
    "    font_size=12,\n",
    "    edge_width=0.5,\n",
    "    edge_alpha=0.9,\n",
    "    figsize=(3, 2.5),\n",
    "    show_labels=False,\n",
    "    layout='kamada_kawai_layout',\n",
    "    node_size=15,\n",
    ")\n",
    "\n",
    "# display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/counts\"\n",
    "counts = pd.DataFrame(dict(\n",
    "    tweets=flatten([load_json(f\"{d}/{f}\", \"tweet_count\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")]),\n",
    "    retweets=flatten([load_json(f\"{d}/{f}\", \"tweet_count\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])),\n",
    "    index=flatten([load_json(f\"{d}/{f}\", \"start\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")], 10),\n",
    ")\n",
    "# counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/errors\"\n",
    "tweet_errors = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")])\n",
    "retweet_errors = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])\n",
    "errors = pd.concat([tweet_errors, retweet_errors]).drop_duplicates(\"resource_id\")\n",
    "errors.index = range(errors.shape[0])\n",
    "# errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suspended users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = errors[errors[\"resource_type\"] == \"tweet\"]\n",
    "u = errors[errors[\"resource_type\"] == \"user\"]\n",
    "u[u[\"parameter\"] == 'in_reply_to_user_id'][\"detail\"].apply(\n",
    "    lambda x: True if \"has been suspended\" in x else None).dropna().shape\n",
    "# u[u[\"parameter\"] == 'entities.mentions.username']\n",
    "# print(t, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/media\"\n",
    "tweet_media = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")])\n",
    "retweet_media = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])\n",
    "media = pd.concat([tweet_media, retweet_media]).drop_duplicates(\"media_key\")\n",
    "media.index = range(media.shape[0])\n",
    "# media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/places\"\n",
    "places = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d))]).drop_duplicates(\"id\")\n",
    "places.index = range(places.shape[0])\n",
    "# places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polls (2016 onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/polls\"\n",
    "tweet_polls = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")])\n",
    "retweet_polls = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])\n",
    "polls = pd.concat([tweet_polls, retweet_polls]).drop_duplicates(\"id\")\n",
    "polls.index = range(polls.shape[0])\n",
    "# polls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referenced tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/tweets\"\n",
    "tweet_ref = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")])\n",
    "retweet_ref = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])\n",
    "ref = pd.concat([tweet_ref, retweet_ref]).drop_duplicates(\"id\")\n",
    "ref.index = range(ref.shape[0])\n",
    "# ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referenced users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"../data/users\"\n",
    "tweet_users = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"t\")])\n",
    "retweet_users = pd.concat([load_json(f\"{d}/{f}\") for f in sorted(os.listdir(d)) if f.startswith(\"r\")])\n",
    "users = pd.concat([tweet_users, retweet_users]).drop_duplicates(\"id\")\n",
    "users.index = range(users.shape[0])\n",
    "# users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated count of published tweets (2013-2018)\n",
    "\n",
    "> Search query ([twitter-v2](https://github.com/nelsonaloysio/twitter_v2_search)): `(Manifestação OR Manifestações OR Protesto OR Protestos OR #VemPraRua) (lang:pt OR place:Brazil OR place_country:BR)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.read_json(\"../json/stats_2013-2018.json\")\n",
    "# _['tweet_count'].plot(figsize=(20,4))\n",
    "_.index = [x.strftime(\"%Y-%m-%d\") for x in _.index]\n",
    "\n",
    "d = pd.DataFrame(index=[x[5:] for x in _.index[:365]])\n",
    "\n",
    "for y in _[\"y\"].unique():\n",
    "    d[y] = _.loc[[f\"{y}-{x}\" for x in d.index], \"tweet_count\"].values\n",
    "\n",
    "colors = [\"#3c76d6\", \"orange\", \"green\", \"red\", \"purple\", \"brown\"]\n",
    "yticks = [750000, 70000, 450000, 300000, 100000, 175000]\n",
    "ax = d.plot(figsize=(9.2, 8.7), subplots=True, style='-')\n",
    "\n",
    "[a.grid(color=\"#eee\") for a in ax]\n",
    "[a.set_yticks([0, int(yticks[i]/2), yticks[i]]) for i, a in enumerate(ax)]\n",
    "#[a.axhline(0.5, color=colors[i], ls='--', linewidth=1.25, alpha=0.75) for i, a in enumerate(ax)]\n",
    "[a.set_ylim([0, yticks[i]]) for i, a in enumerate(ax)]\n",
    "[a.set_xlim([0, 365]) for a in ax]\n",
    "[a.set_xticks([i for i, x in enumerate(d.index) if x.endswith(\"-01\")] + [365]) for a in ax]\n",
    "\n",
    "[ax[-1].set_xticklabels([\"1/Jan\", \"Fev\", \"Mar\", \"Abr\", \"Mai\", \"Jun\", \"Jul\", \"Ago\", \"Set\", \"Out\", \"Nov\", \"Dez\", \"31/Dez\"])]\n",
    "\n",
    "ax[2].set_ylabel(\"Número diário de publicações no Twitter com um ou mais termos selecionados                                 \",\n",
    "                 loc=\"center\", fontsize=10)\n",
    "\n",
    "fmtr = FuncFormatter(lambda x, p: format(int(x), ',').replace(',', '.'))\n",
    "[a.yaxis.set_major_formatter(fmtr) for a in ax]\n",
    "plt.minorticks_off()\n",
    "# [d[d.columns[i]].max() for i in range(d.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Daily (incl. \"`manifestante`\", \"`manifestantes`\")\n",
    "\n",
    "> Comparatively, in the chart below, there's a lot of added noise in the years 2014 and 2017 due to the inclusion of the keywords above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = pd.read_json(\"../json/stats_2013_2018_new.json\")\n",
    "ax = d_.plot(subplots=True, figsize=(9, 8))  # , style='.-')\n",
    "[a.set_xlim([0, 365]) for a in ax]\n",
    "[a.grid(color=\"#eee\") for a in ax]\n",
    "plt.minorticks_off()\n",
    "d_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = [2914867, 176551, 632738, 668321, 186084, 317893]\n",
    "g = _.groupby(\"w\").sum()[\"tweet_count\"]\n",
    "idx = g.index.str[5:].unique()\n",
    "\n",
    "d = pd.DataFrame()\n",
    "for y in _[\"y\"].unique():\n",
    "    rng = idx[1:] if y == 2017 else idx[:53]\n",
    "    d[y] = g.loc[[f\"{y}-{x}\" for x in rng]].values\n",
    "\n",
    "colors = [\"#3c76d6\", \"orange\", \"green\", \"red\", \"purple\", \"brown\"]\n",
    "colors_ = [\"#03558d\", \"#d86500\", \"green\", \"#d62728\", \"purple\", \"#8c574c\"]\n",
    "\n",
    "ax = d.plot(figsize=(9.2, 8.7), subplots=True, style='.-')\n",
    "\n",
    "[a.grid(color=\"#eee\") for a in ax]\n",
    "# [a.axhline(d.iloc[:, i].mean(), color=colors[i], ls='--', linewidth=1.25, alpha=0.75) for i, a in enumerate(ax)]\n",
    "# ticks = _[_.index.str.endswith(\"-01\")][\"w\"].str[5:][:12].values.tolist()\n",
    "\n",
    "ticks = {i: [int(x) for x in _[_.index.str.endswith(\"-01\")].query(f\"y == {y}\")[\"w\"].str[5:].values.tolist()] for i,y in enumerate(_[\"y\"].unique())}\n",
    "[a.set_xticks(ticks[i]) for i,a in enumerate(ax)]\n",
    "[ax[-1].set_xticklabels([\"Jan\", \"Fev\", \"Mar\", \"Abr\", \"Mai\", \"Jun\", \"Jul\", \"Ago\", \"Set\", \"Out\", \"Nov\", \"Dez\"])]\n",
    "[a.set_xlim([0, 52]) for a in ax]\n",
    "[a.set_yticks([0,  d.iloc[:, i].max()/2,   d.iloc[:, i].max()]) for i, a in enumerate(ax)]\n",
    "[a.get_yticklabels()[1].set_color(colors_[i]) for i, a in enumerate(ax)]\n",
    "[a.get_yticklabels()[2].set_color(colors_[i]) for i, a in enumerate(ax)]\n",
    "\n",
    "# [a.plot(range(56), [d.iloc[:, i].max()/2 for _ in range(56)], linestyle='--', linewidth='1.25', color=colors[i], alpha=0.5) for i, a in enumerate(ax)]\n",
    "fmtr = FuncFormatter(lambda x, p: format(int(x), ',').replace(',','.'))\n",
    "[a.yaxis.set_major_formatter(fmtr) for a in ax]\n",
    "ax[3].set_ylabel(\"                           Total de publicações por semana no Twitter com um ou mais termos de pesquisa selecionados\",\n",
    "                 loc=\"center\", fontsize=10)\n",
    "plt.minorticks_off()\n",
    "[d[d.columns[i]].max() for i in range(d.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = _.groupby(\"m\").sum()[\"tweet_count\"]\n",
    "idx = g.index.str[5:].unique()\n",
    "\n",
    "d = pd.DataFrame()\n",
    "for y in _[\"y\"].unique():\n",
    "    rng = idx  # [1:] if y == 2017 else idx[:53]\n",
    "    d[y] = g.loc[[f\"{y}-{x}\" for x in rng]].values\n",
    "\n",
    "colors = [\"#3c76d6\", \"orange\", \"green\", \"red\", \"purple\", \"brown\"]\n",
    "ax = d.plot(figsize=(9.2, 8.7), subplots=True, style='.-')\n",
    "[a.grid(color=\"#eee\") for a in ax]\n",
    "# [a.axhline(d.iloc[:, i].mean(), color=colors[i], ls='--', linewidth=1.25, alpha=0.75) for i, a in enumerate(ax)]\n",
    "[a.set_yticks([0, d.iloc[:, i].max()/2, d.iloc[:, i].max()]) for i, a in enumerate(ax)]\n",
    "[a.set_xlim([0, 11]) for a in ax]\n",
    "[a.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) for a in ax]\n",
    "[ax[-1].set_xticklabels([\"Jan\", \"Fev\", \"Mar\", \"Abr\", \"Mai\",\n",
    "                        \"Jun\", \"Jul\", \"Ago\", \"Set\", \"Out\", \"Nov\", \"Dez\"])]\n",
    "ax[2].set_ylabel(\"Número mensal de publicações no Twitter com um ou mais termos selecionados                                 \",\n",
    "                 loc=\"center\", fontsize=10)\n",
    "fmtr = FuncFormatter(lambda x, p: format(int(x), ',').replace(',', '.'))\n",
    "[a.yaxis.set_major_formatter(fmtr) for a in ax]\n",
    "plt.minorticks_off()\n",
    "[d[d.columns[i]].max() for i in range(d.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.groupby(\"y\").describe()[\"tweet_count\"].T.applymap(lambda x: f\"{int(x)}\")\n",
    "_.groupby(\"y\").sum().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional #1: June 2013\n",
    "\n",
    "> `protesto OR protestos OR dilma OR manifestação OR manifestaçoẽs OR #vemprarua OR impeachment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.read_json('../json/stats_2013.json')\n",
    "_.loc[:, _.sum().sort_values(ascending=False)[:5].index].plot(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional #2: 2013-2016\n",
    "\n",
    "> `impeachment+dilma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.Series(json.loads(\n",
    "    open(\"../json/stats_2013-2016_impeachment+dilma.json\").read()))\n",
    "_.plot(figsize=(20,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social network usage (2013-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../json/social_networks.json')\n",
    "df = df.filter(['Facebook','YouTube','Twitter','Tumblr','Pinterest'])#,'Instagram'])\n",
    "df.columns = ['Facebook','YouTube','Twitter','Tumblr','Pinterest']#,'Instagram*']\n",
    "# df.index = range(df.shape[0])\n",
    "ax = df.plot(figsize=(7.9,4.2), alpha=1)\n",
    "\n",
    "x = range(df.shape[0]+1000)\n",
    "plt.plot([x for x in x], [int(df.iloc[:, 0].mean()) for x in x], linestyle='--', linewidth='1.25', color='#3c76d6', alpha=0.5)\n",
    "plt.plot([x for x in x], [int(df.iloc[:, 2].mean()) for x in x], linestyle='--', linewidth='1.25', color='g', alpha=0.5)\n",
    "ax.set_yticks([4.24,15,30,45,60,76.41,90])\n",
    "ax.set_yticklabels(['4.24%','15%', '30%', '45%', '60%','76.41%','90%'])\n",
    "ax.set_ylim([0,100])\n",
    "# fmtr = StrMethodFormatter('{x:2.0f}%')\n",
    "# ax.yaxis.set_major_formatter(fmtr)\n",
    "\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.15)\n",
    "plt.ylabel('Estimativa de utilização da plataforma entre brasileiros')\n",
    "plt.legend(fontsize=12, title='Plataforma (>1%)')#, bbox_to_anchor=(1, 1.025), loc='upper left', fontsize=12)\n",
    "ax.get_lines()[0].set_alpha(1.0)\n",
    "ax.get_lines()[1].set_alpha(0.7)\n",
    "ax.get_lines()[2].set_alpha(1)\n",
    "ax.get_lines()[3].set_alpha(0.7)\n",
    "ax.get_lines()[4].set_alpha(0.7)\n",
    "# ax.get_lines()[5].set_alpha(0.2)\n",
    "\n",
    "ax.get_yticklabels()[0].set_color(\"#03558d\")\n",
    "ax.get_yticklabels()[5].set_color(\"#3c76d6\")\n",
    "plt.minorticks_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protests (2015-2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../json/time_series_2015_2016.json')\n",
    "df.columns = ['a) Oposição (pró-impeachment)', 'b) Situação (contra o impeachment)']\n",
    "ax = df.plot(kind='bar', color=['#3c76d6', 'r'], figsize=(10, 4), rot=0)\n",
    "x = range(df.shape[0]+10)\n",
    "plt.plot([x-1 for x in x], [int(df.iloc[:, 0].mean()) for x in x], linestyle='--', linewidth='1.25', color='#3c76d6', alpha=0.75)\n",
    "plt.plot([x-1 for x in x], [int(df.iloc[:, 1].mean()) for x in x], linestyle='--', linewidth='1.25', color='r', alpha=0.75)\n",
    "\n",
    "ax.set_xticklabels(['Março de 2015', 'Abril de 2015', 'Agosto de 2015', 'Dezembro de 2015', 'Março de 2016', 'Abril de 2016', 'Ago./out. de 2016'])\n",
    "ax.set_yticks([0,91285,500000,1000000,1146428,1500000,2000000,2500000,3000000,3500000,4000000])\n",
    "ax.set_yticklabels(['0','0.09', '0.5', '1','1.15','1.5','2','2.5','3','3.5','4'], fontsize=10)\n",
    "plt.grid(color = 'gray', linestyle = '--', axis='y', linewidth = 0.15)\n",
    "plt.legend(title='Est. de manifestantes em protestos de 2015-2016', loc='upper left', fontsize=12)\n",
    "plt.ylabel('Número de manifestantes (em milhões)')\n",
    "ax.set_ylim([1, 3800000])\n",
    "\n",
    "ax.get_yticklabels()[1].set_color(\"r\")\n",
    "ax.get_yticklabels()[4].set_color(\"#3c76d6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini (1960-2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../json/gini.json')\n",
    "df.index = [' '+x+' ' for x in df.index.astype(str).values]\n",
    "deg = df.index.tolist()\n",
    "cnt = [x for x in df.values.tolist() for x in x]\n",
    "fig, ax = plt.subplots(figsize=(9.6,3.4),)\n",
    "\n",
    "plt.bar(deg, cnt, width=0.5, color='#3c76d6')\n",
    "plt.ylabel(\"Índice de Gini\", fontsize=12)\n",
    "plt.axes.fontsize = 14\n",
    "ax.set_xticks([d for d in deg])\n",
    "ax.set_xticklabels([d for d in deg], fontsize=12)\n",
    "ax.set(ylim=[0.45, 0.65])\n",
    "ax.set_yticks([0.45, 0.5, 0.55, 0.5645, 0.6])\n",
    "ax.set_yticklabels(['0.45', '0.50', '0.55', '0.5645', '0.60'])\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.15, axis='y')\n",
    "plt.plot()\n",
    "\n",
    "rects = ax.patches\n",
    "labels = [x for x in df.values.tolist() for x in x]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    x = rect.get_x() + rect.get_width() / 2\n",
    "    h = rect.get_height() + 0.001\n",
    "    ax.text(x, h, f'{label:.4f}', fontsize=12,\n",
    "            ha='center', va='bottom')\n",
    "    \n",
    "x = range(df.shape[0]+10)\n",
    "ax.set_xlim([-0.5, 7.5])\n",
    "ax.set_ylim([0.45, 0.63])\n",
    "ax.get_yticklabels()[3].set_color(\"#3c76d6\")\n",
    "plt.plot([x-1 for x in x], [df.iloc[:, 0].mean() for x in x], linestyle='--', linewidth='1.25', color='#3c76d6', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political parties (2010-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../json/party_2010_2018.json')\n",
    "ax = df.plot(kind='line', figsize=(4.2,4.2), style='.-')\n",
    "ax.set(xticks=df.index)\n",
    "ax.set(xlim=[df.index[0], df.index[-1]])\n",
    "fmtr = StrMethodFormatter('{x:2.1f}%')\n",
    "ax.yaxis.set_major_formatter(fmtr)\n",
    "ax.set_xticklabels(df.index, fontsize=10)\n",
    "\n",
    "plt.legend(title='Partido', bbox_to_anchor=(1, 1.03), loc='upper left', fontsize=12)\n",
    "plt.grid(color = 'gray', linestyle = '--', axis='y', linewidth = 0.15)\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.15)\n",
    "plt.ylabel(\"Porcetagem de deputados federais eleitos\", fontsize=12)\n",
    "\n",
    "[x.set_alpha(0.75) for x in ax.get_lines()]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement/interactions (2013-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../json/metrics.json\", \"r\") as j:\n",
    "    m = {int(k): pd.DataFrame(v) for k, v in json.loads(j.read()).items()}\n",
    "    for y in m:\n",
    "        print(y, m[y].iloc[:, 1:].sum().sum() / m[y].iloc[:, 0].sum().sum())\n",
    "\n",
    "df_ = pd.DataFrame({k: v.sum().to_dict() for k, v in m.items()}).T.apply(lambda x: x*100/sum(x), axis=1)\n",
    "df_.index = [\"$\\it{T}$$_%s$ (%s)\" % (i,k) for i,k in enumerate(df_.index)]\n",
    "ax = df_.plot(kind=\"bar\", figsize=(7,3.04), stacked=True, width=0.54)\n",
    "fmtr = FuncFormatter(lambda x, p: format(int(x), ',').replace(',', '.')+'%')\n",
    "ax.yaxis.set_major_formatter(fmtr)\n",
    "#ax.set_xticklabels(df.index, fontsize=10)\n",
    "ax.set_ylabel(\"Total de interações por tipo (%)\", fontsize=12)\n",
    "ax.set_ylim([0, 100])\n",
    "# plt.legend(title='Interações', title_fontsize=12, \n",
    "#             bbox_to_anchor=(1,1.027), \n",
    "#             loc='upper left',\n",
    "#             fontsize=12, ncol=1, columnspacing=1)\n",
    "plt.legend(ncol=5, bbox_to_anchor=(0.434,-.322), loc='lower center', columnspacing=.1, fontsize=12)\n",
    "# plt.legend(title_fontsize=12, bbox_to_anchor=(0.5, -0.4), loc='lower center', fontsize=12, columnspacing=1, ncol=2)\n",
    "#plt.grid(color = 'gray', linestyle = '--', axis='y', linewidth = 0.15)\n",
    "#plt.grid(color = 'gray', linestyle = '--', linewidth = 0.15)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.35)\n",
    "plt.xticks(rotation='horizontal')\n",
    "\n",
    "pd.DataFrame({k: v.sum().to_dict() for k, v in m.items()}).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmtr = FuncFormatter(lambda x, p: format(int(x), ',').replace(',', '.'))\n",
    "# matplotlib.rc('text', usetex = True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2.5))\n",
    "plotstats(m, 121, 2013, [0, 500000], fmtr)\n",
    "plt.ylabel('Total de interações')\n",
    "plotstats(m, 122, 2014, [0, 40000], fmtr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2.5))\n",
    "plotstats(m, 121, 2015, [0, 350000], fmtr)\n",
    "plt.ylabel('Total de interações')\n",
    "plotstats(m, 122, 2016, [0, 300000], fmtr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2.5))\n",
    "plotstats(m, 121, 2017, [0, 200000], fmtr, legend=True)\n",
    "plt.ylabel('Total de interações')\n",
    "plotstats(m, 122, 2018, [0, 500000], fmtr)\n",
    "\n",
    "# plt.legend(title_fontsize=12, bbox_to_anchor=(0.5, -0.4),\n",
    "#             loc='lower center', fontsize=12, columnspacing=1, ncol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet source per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plots(d, y=None, key=None, kind=\"barh\"):\n",
    "#     for y in (y if type(y) == list else [y] if type(y) == str else d.keys()):\n",
    "#         fig = plt.figure()\n",
    "#         ax = (d[y][key] if key else d[y]).value_counts(ascending=False)[:15].sort_values().plot(kind=kind)\n",
    "#         ax.set_title(f\"{y}\")\n",
    "#         ax.plot()\n",
    "# plots(source, [2013,2014,2015,2016,2017,2018], \"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing artificial network models\n",
    "\n",
    "Quick comparison between Erdös-Rényi (random), Watts-Strogatz (small-word) and Barabási-Albert (scale-free) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.erdos_renyi_graph(50, 0.1, seed=20)\n",
    "f1 = nx_plot(G, **params, title=f'a) Erdös-Rényi (p=0.1)', colors='r')\n",
    "graph_histogram(G, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.watts_strogatz_graph(50, k=4, p=0.1, seed=85)\n",
    "f2 = nx_plot(G, **params, title=f'b) Watts-Strogatz (k=4, p=0.1)', colors='#3c76d6')\n",
    "graph_histogram(G, color='#3c76d6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.barabasi_albert_graph(50, m=1, seed=333)\n",
    "f3 = nx_plot(G, **params, title=f'c) Barabási-Albert (m=1)', colors='g')\n",
    "graph_histogram(G, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(\"full\"):\n",
    "    if f.endswith(\".json\") and f.startswith(\"retweets_\"):\n",
    "        extract_graph(f\"full/{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build main graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in reversed(sorted(os.listdir(path))):\n",
    "    if f.endswith(\"_full.csv\"):\n",
    "        print(f)\n",
    "        G = gk.graph(f\"graphs/{f}\", source_attr=\"source\", target_attr=\"target\", directed=True, weights=True)\n",
    "        G = gk.nx_set_node_attrs(G, pd.read_csv(f\"graphs/{f}\".replace(\"_full\", \"_nodes\"), index_col=\"id\"))\n",
    "        gk.nx_write_graph(G, f\"{f}.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build daily graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in reversed(sorted(os.listdir(path))):\n",
    "#     if f.endswith(\"_nodes.csv\"):\n",
    "#         year = f[15:19]\n",
    "#         nodes = pd.read_csv(f\"{path}/{f}\", index_col=\"id\")\n",
    "#         edges = pd.read_csv(f\"{path}/{f}\".replace(\"_nodes\",\"_edges\"))\n",
    "#         for x in sorted(os.listdir(f\"{path}/{year}\")):\n",
    "#             if x.endswith(\"_nodes.csv\"):\n",
    "#                 print(x)\n",
    "#                 nodes_daily = pd.read_csv(f\"{path}/{year}/{x}\", index_col=\"id\")\n",
    "#                 nodes_daily[\"leiden\"] = nodes.loc[nodes_daily.index, \"leiden\"].astype(str)\n",
    "#                 G = gk.graph(f\"{path}/{year}/{x}\".replace(\"_nodes\",\"_full\"), source_attr=\"source\", target_attr=\"target\", directed=True, weights=True)\n",
    "#                 G = gk.nx_set_node_attrs(G, nodes_daily)\n",
    "#                 gk.nx_write_graph(G, f\"{path}/{year}/{x}.gml\".replace(\"_nodes.csv\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build filtered graphs (by in-degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in reversed(sorted(os.listdir(path))):\n",
    "#     if f.startswith(\"graph_\"):\n",
    "#         year = f.split(\"_\")[1].split(\".\")[0]\n",
    "#         G = gk.graph(f\"graphs/{f}\")\n",
    "#         nodes = gk.nodes(G)\n",
    "#         in_deg = int(nodes[\"in_degree\"].mean())+1\n",
    "#         G_ = gk.subgraph(G, nodes[nodes[\"in_degree\"] >= in_deg].index)\n",
    "#         print(year, G_.order(), G_.size())\n",
    "#         gk.nx_write_graph(G_, f\"graphs/filtered_{year}_indeg={in_deg}.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load nodes, edges, centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"graphs\"\n",
    "\n",
    "nodes = {} # yearly\n",
    "nodes_ = {} # daily\n",
    "source = {} # yearly\n",
    "target = {} # yearly\n",
    "\n",
    "for f in sorted(os.listdir(path)):\n",
    "    if f.startswith(\"centrality_\"):\n",
    "        year = int(f.split(\"_\")[1].replace(\".csv\", \"\"))\n",
    "        nodes[year] = pd.read_csv(f\"{path}/{f}\", index_col=\"label\")\n",
    "        nodes_[year] = pd.DataFrame(index=nodes[year].index.values.tolist())\n",
    "        nodes[year].index.name = 'id'\n",
    "    if f.startswith(\"edges_\"):\n",
    "        source[year] = pd.read_csv(f\"{path}/{f}\".replace(\"_nodes\", \"_edges\"), index_col=\"source\", usecols=[\"source\"]).dropna()\n",
    "        target[year] = pd.read_csv(f\"{path}/{f}\".replace(\"_nodes\", \"_edges\"), index_col=\"target\", usecols=[\"target\"]).dropna()\n",
    "\n",
    "all_nodes = pd.DataFrame(pd.concat([pd.Series(x.index) for x in nodes_.values()]).value_counts())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare source and target nodes\n",
    "\n",
    "> Ao todo, foram observados 604.540 atores; destes, 549.717 (90,93%) foram constatados compartilhando publicações realizadas por outros perfis na rede, para um total de 140.686 (23,27%) membros sendo republicados em todos os intervalos observados. Apenas 85.863 (14,2%) do total foi observado realizando as duas ações, isto é, tanto compartilhando conteúdo quanto tendo seu conteúdo compartilhado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = pd.Index(pd.concat([pd.Series(x.index) for x in nodes.values()])).drop_duplicates()\n",
    "all_sources = pd.Index(pd.concat([pd.Series(x.index) for x in source.values()])).drop_duplicates()\n",
    "all_targets = pd.Index(pd.concat([pd.Series(x.index) for x in target.values()])).drop_duplicates()\n",
    "\n",
    "f\"{all_nodes.shape[0]} nodes ({all_sources.shape[0]} [source] => {all_targets.shape[0]} [target]) (source & target amounts to {all_sources.intersection(all_targets).shape[0]} nodes)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_gram(nodes=nodes):\n",
    "\n",
    "    def hist_freq(_):\n",
    "        nodes = _.index.tolist()\n",
    "        values = _.values.tolist()\n",
    "        dmax = max(values)+1\n",
    "        freq = [0 for d in range(dmax)]\n",
    "        for d in values:\n",
    "            freq[d] += 1\n",
    "        return freq\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9.5, 6))\n",
    "    # plt.figure(figsize=(9.5, 6))\n",
    "    plt.subplot(231)\n",
    "    df_ = hist_freq(nodes[2013][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='2013')\n",
    "    df_ = hist_freq(nodes[2013][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='2013*')\n",
    "    plt.ylabel('Frequência (número de nós)')\n",
    "    plt.title(\"$\\it{T}$$_0$ (2013)\")\n",
    "    plt.subplot(232)\n",
    "    df_ = hist_freq(nodes[2014][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='Grau de entrada')\n",
    "    df_ = hist_freq(nodes[2014][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='Grau de saída')\n",
    "    plt.title(\"$\\it{T}$$_1$ (2014)\")\n",
    "    # plt.legend(title_fontsize=12, bbox_to_anchor=(0.5, -0.4), loc='lower center', fontsize=12, columnspacing=1, ncol=2)\n",
    "    plt.subplot(233)\n",
    "    df_ = hist_freq(nodes[2015][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='2015')\n",
    "    df_ = hist_freq(nodes[2015][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='2015*')\n",
    "    plt.title(\"$\\it{T}$$_2$ (2015)\")\n",
    "    fig, ax = plt.subplots(figsize=(9.5, 6))\n",
    "    # plt.figure(figsize=(9.5, 6))\n",
    "    plt.subplot(231)\n",
    "    df_ = hist_freq(nodes[2016][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='2013')\n",
    "    df_ = hist_freq(nodes[2016][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='2013*')\n",
    "    plt.ylabel('Frequência (número de nós)')\n",
    "    plt.title(\"$\\it{T}$$_3$ (2016)\")\n",
    "    plt.subplot(232)\n",
    "    df_ = hist_freq(nodes[2017][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='Grau de entrada')\n",
    "    df_ = hist_freq(nodes[2017][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='Grau de saída')\n",
    "    plt.title(\"$\\it{T}$$_4$ (2017)\")\n",
    "    plt.legend(title_fontsize=12, bbox_to_anchor=(0.5, -0.4), loc='lower center', fontsize=12, columnspacing=1, ncol=2,)\n",
    "    plt.subplot(233)\n",
    "    df_ = hist_freq(nodes[2018][\"in_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'b-', label='2018')\n",
    "    df_ = hist_freq(nodes[2018][\"out_degree\"])\n",
    "    plt.loglog(range(len(df_)), df_, 'g-', label='2018*')\n",
    "    plt.title(\"$\\it{T}$$_5$ (2018)\")\n",
    "\n",
    "hist_gram()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix (Jaccard indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_corr(dct, **kwargs):\n",
    "    d1 = pd.DataFrame({\n",
    "        key: {\n",
    "            k: df.index.intersection(dct[k].index).drop_duplicates().shape[0] for k in dct}\n",
    "        for key, df in dct.items()\n",
    "    })\n",
    "    d2 = d1.divide(d1.max(), axis=0)\n",
    "    sns.set(font_scale=1.0)\n",
    "    sns.set(font=\"Times New Roman\")\n",
    "    # Sample figsize in inches\n",
    "    fig, ax = plt.subplots(figsize=(5.3, 4.3))\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "    heatmap(d2.round(2), annot=True, center=.5,  linewidths=0, ax=ax, **kwargs)\n",
    "    ax.set_xticklabels([\"$\\it{T}$$_%s$\" % i for i in range(6)], fontsize=14)\n",
    "    ax.set_yticklabels([\"$\\it{T}$$_%s$\" % i for i in range(6)], fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    # .applymap(lambda x: x) + d2.multiply(100).round(2).applymap(lambda x: f' ({x}%)'.replace('.0%','%').replace(\".\",\",\")).astype(str)\n",
    "    return d1.astype(str)\n",
    "\n",
    "\n",
    "gen_user_corr(nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "leiden = {}\n",
    "nodes = {}\n",
    "\n",
    "for f in sorted(os.listdir(path)):\n",
    "    p = f\"{path}/{f}\"\n",
    "    if f.startswith(\"graph_\"):\n",
    "        year = int(f.split(\"_\")[1].split(\".\")[0])\n",
    "        G = gk.graph(p)\n",
    "        graphs[year] = G\n",
    "        nodes[year] = gk.nodes(G)\n",
    "        leiden[year] = nodes[year][\"leiden\"]\n",
    "        leiden[year].index = G.nodes()\n",
    "\n",
    "stats = pd.DataFrame({year: {c: f\"{_.loc['mean',c]} ± {_.loc['std',c]}\" for c in _.columns} for year, _ in \n",
    " {k: gk.nodes(graphs[k]).describe().round(2).loc[['mean','std']].astype(str) for k in sorted(graphs.keys())}.items()})\n",
    "stats.drop([\"leiden\", \"louvain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = {}\n",
    "for year in sorted(graphs):\n",
    "    G = graphs[year]\n",
    "    density[year] = nx.density(G)\n",
    "    dens = str(density[year])\n",
    "    print(year, f\"{dens[:4]}{dens[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coreness / k-cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_loops(G):\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "k_cores = stats.loc[\"degree\"].apply(lambda x: int(x[0])+1).to_dict()\n",
    "k_graphs = {y: nx.k_core(remove_self_loops(graphs[y]), k) for y,k in k_cores.items()}\n",
    "print(\"order:\", [x.order() for x in k_graphs.values()])\n",
    "k_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"graphs\"\n",
    "\n",
    "graphs = {}\n",
    "leiden = {}\n",
    "nodes = {}\n",
    "\n",
    "for f in sorted(os.listdir(path)):\n",
    "    p = f\"{path}/{f}\"\n",
    "    if f.startswith(\"graph_\"):\n",
    "        year = int(f.split(\"_\")[1].split(\".\")[0])\n",
    "        G = gk.graph(p)\n",
    "        graphs[year] = G\n",
    "        nodes[year] = gk.nodes(G)\n",
    "        leiden[year] = nodes[year][\"leiden\"]\n",
    "        leiden[year].index = G.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leiden modules/communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import leidenalg as la\n",
    "\n",
    "# modules = {}\n",
    "# results = {}\n",
    "\n",
    "# for y in sorted(graphs):\n",
    "#     iG = gk.nx2ig(graphs[y])\n",
    "#     mod = la.find_partition(iG, la.ModularityVertexPartition, seed=0, n_iterations=10)\n",
    "    \n",
    "#     communities = max(mod.membership)+1 if mod.membership else 0\n",
    "#     modularity = mod.quality()\n",
    "\n",
    "#     print(y)\n",
    "#     print(f'Communities (Leiden): {communities} (m={modularity:.3f})')\n",
    "\n",
    "#     modules[y] = pd.Series(\n",
    "#         pd.to_numeric(mod.membership, downcast='integer'),\n",
    "#         name='leiden_partition',\n",
    "#     )\n",
    "#     results[y] = mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leiden temporal communities\n",
    "\n",
    "> Instead of computing the communities considering each temporal graph as above, let's consider all periods at once with an `interslice_weight` of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igraphs = {}\n",
    "\n",
    "for y in sorted(graphs):\n",
    "    G = graphs[y]\n",
    "    iG = ig.Graph(directed=G.is_directed())\n",
    "    \n",
    "    n_ = gk.nodes(G)\n",
    "    e_ = gk.edges(G)\n",
    "\n",
    "    iG.add_vertices(n_.index.tolist())\n",
    "    iG.vs['id'] = n_.index.tolist()\n",
    "    \n",
    "    iG.add_edges(e_[[\"source\", \"target\"]].values.tolist())\n",
    "    iG.es[\"weight\"] = e_[\"weight\"]\n",
    "    \n",
    "    igraphs[y] = iG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership, improvement = la.find_partition_temporal(\n",
    "    list(igraphs.values()),\n",
    "    la.ModularityVertexPartition,\n",
    "    interslice_weight=1,\n",
    "    seed=0,\n",
    "    n_iterations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden = {\n",
    "    year:\n",
    "        pd.Series(\n",
    "            membership[i],\n",
    "            index=graphs[year].nodes(),\n",
    "            name='leiden',\n",
    "        ).astype(int) # .to_dict()\n",
    "    for i, year in enumerate(graphs.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot top communities over time (order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leiden_ = pd.DataFrame(leiden)\n",
    "# leiden_ = {y: leiden[y].value_counts().divide(leiden[y].value_counts().sum()) for y in leiden}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_ = {}\n",
    "top_leiden = pd.concat(pd.Series(df.value_counts().index[:5]) for df in leiden.values()).unique().tolist()\n",
    "max_leiden = pd.concat(pd.Series(df.unique()) for df in leiden.values()).unique().max()\n",
    "\n",
    "for y, m in leiden.items():\n",
    "    leiden_[y] = {\n",
    "        **{\"$\\it{C}$$_{%s}$\"%(c+1 if c != 38 else 7): m.value_counts().loc[c] / m.value_counts().sum() for i,c in enumerate(m.value_counts().index.intersection(top_leiden))},\n",
    "        **{\"$\\it{C}$$_{8}$ - $\\it{C}$$_{25}$\": m.value_counts().drop(top_leiden).iloc[:18].sum() / m.value_counts().sum()},\n",
    "        **{\"$\\it{C}$$_{26}$ - $\\it{C}$$_{n}$\": m.value_counts().drop(top_leiden).iloc[18:].sum() / m.value_counts().sum()},\n",
    "    }\n",
    "leiden_ = pd.DataFrame({\"$\\it{T}$$_{%s}$ (%s)\" % (i,k): leiden_[k] for i, k in enumerate(leiden_)})\n",
    "\n",
    "plt.figure()\n",
    "ax = leiden_.T.plot(kind=\"bar\", stacked=True, figsize=(7.6,3.6), width=0.56, fontsize=12 )\n",
    "ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%', '100%', ])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.legend(ncol=1, loc='upper right', title='Comunidade ($\\it{C}$)',  bbox_to_anchor=(1.275,1.04), columnspacing=1.4, fontsize=13, title_fontsize=12)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.35)\n",
    "plt.xticks(rotation='horizontal')\n",
    "# ax.tick_params(axis='y', colors='blue')\n",
    "# [[x.set_facecolor(\"#999\") for x in ax.containers[-3]]]\n",
    "[[x.set_facecolor(\"#bbb\") for x in ax.containers[-2]]]\n",
    "[[x.set_facecolor(\"#ddd\") for x in ax.containers[-1]]]\n",
    "# ax.get_legend().legendHandles[-3].set_facecolor('#999')\n",
    "ax.get_legend().legendHandles[-2].set_facecolor('#bbb')\n",
    "ax.get_legend().legendHandles[-1].set_facecolor('#ddd')\n",
    "ax.set_ylabel(\"Total de perfis por comunidade (%)\", fontsize=12)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "\n",
    "# for _, __ in leiden.items():\n",
    "#     print(_, [x+1 if x != 38 else 7 for x in __.value_counts().index[:5]])\n",
    "leiden_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List top communities' most relevant nodes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in leiden:\n",
    "    print(year)\n",
    "    G = graphs[year]\n",
    "    df = leiden[year]\n",
    "    cent = gk.compute(G, \"degree\")\n",
    "    for c in top_leiden:\n",
    "        c_ = cent.loc[df[df==c].index]\n",
    "        print(int(str(c).replace(\"38\",\"6\"))+1, c_.shape[0], 'nós', '\\t', c_.sum().values[0], 'arestas',\n",
    "              c_[\"degree\"].sort_values(ascending=False).index[:10].tolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Louvain and Leiden communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for y in nodes:\n",
    "#     fig = plt.figure()\n",
    "#     leiden = nodes[y]['leiden'].value_counts().values\n",
    "#     louvain = nodes[y]['louvain'].value_counts().values\n",
    "#     pd.DataFrame({\"leiden\": leiden[:100], \"louvain\": louvain[:100]}).plot(figsize=(14,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community significance with [qstest](https://github.com/skojaku/qstest)\n",
    "\n",
    "> Kojaku, S. and Masuda, N. \"A generalised significance test for individual communities in networks\". Sci. Rep. 8, 7351 (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qsresults = qstest(network, communities, qfunc=, sfunc=, cdalgorithm=, num_of_rand_net=500, alpha=0.05, num_of_thread=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign node colors (per community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in [2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    c = gk.nodes(gk.graph(f\"graphs/f/filtered_{y}.gexf\"))[\"leiden\"].apply(lambda x: COLORS_COMMUNITIES.get(int(x), \"#aaaaaa\"))\n",
    "    c.index.name = \"id\"\n",
    "    c.name = \"color\"\n",
    "    c.to_csv(f\"graphs/colors_{y}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs from C(n), n=7 communities\n",
    "\n",
    "> Restrict temporal graph plotting to the nodes within top 7 Leiden communities previously identified, with a minimum node in-degree equivalent to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"graphs\"\n",
    "\n",
    "graphs = {}\n",
    "indeg = {}\n",
    "pos = {}\n",
    "\n",
    "for f in sorted(listdir(path)):\n",
    "    if f.endswith(\".gexf\") and f.startswith(\"filtered_\"):\n",
    "        year = int(f.split(\"_\")[1].split(\".\")[0])\n",
    "        graphs[year] = gk.graph(f\"{path}/{f}\")\n",
    "        indeg[year] = int(f.split(\"=\")[1].split(\".\")[0])\n",
    "\n",
    "for k in [2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    with open(f\"positions_{k}.json\", \"r\") as j:\n",
    "        pos[k] = pd.DataFrame(json.load(j)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DataShader (graph drawing), KDEEB (kernel-density edge estimation bundling), ForceAtlas2 (node positioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsplot = DataShaderPlot()\n",
    "\n",
    "figs = []\n",
    "\n",
    "order = {}\n",
    "size = {}\n",
    "\n",
    "for year, G in graphs.items():\n",
    "    G = gk.subgraph(G, pos[year].index)\n",
    "    # G = gk.subgraph(G, list(nx.connected_components(G.to_undirected()))[0])\n",
    "    nodes = gk.nodes(G)\n",
    "    cat = nodes[\"leiden\"].astype(\"category\")\n",
    "    nx.set_node_attributes(G, cat, 'cat')\n",
    "    \n",
    "    # G = gk.subgraph(G, list(nx.connected_components(G.to_undirected()))[0]) # <-- first component only\n",
    "    # nodes = gk.nodes(G)\n",
    "    idx = nodes.sort_values([\"leiden\", \"page_rank\"], ascending=[True, False]).index\n",
    "\n",
    "    # pos = gk.circular_layout(idx)\n",
    "    # pos = gk.forceatlas2_layout(G, pos=pos, iterations=100, linlog=False, nohubs=False, seed=0)\n",
    "    # pos = {node:[pos['x'][i],pos['y'][i]] for i,node in enumerate(idx)}\n",
    "    name = f\"{year}, n={G.order()}, E={G.size()}, d={indeg[year]}\"\n",
    "    \n",
    "    order[year] = G.order()\n",
    "    size[year] = G.size()\n",
    "    \n",
    "    print(\"Plotting\", name)\n",
    "    fig = dsplot.ds_plot(\n",
    "        G,\n",
    "        pos=pos[year],\n",
    "        cat=\"cat\",\n",
    "        method=\"bundle\",\n",
    "        bw=.05,\n",
    "        name=name,\n",
    "        output=f\"fig_{year}\",\n",
    "        kwargs=dict(plot_height=768, plot_width=768),\n",
    "    )\n",
    "    figs.append(fig)\n",
    "    \n",
    "dsplot.tf_plot(figs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export figures to images (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = [2013, 2014, 2015, 2016, 2017, 2018]\n",
    "# for i, graph in enumerate(figs):\n",
    "#     ds.utils.export_image(img=graph, filename=f\"fig_{g[i]}\", fmt='.png', background='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add title to figures using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    2013: [12736, 73688],\n",
    "    2014: [1347, 4401],\n",
    "    2015: [4525, 40369],\n",
    "    2016: [4281, 34384],\n",
    "    2017: [1651, 6574],\n",
    "    2018: [1496, 7217],\n",
    "}\n",
    "\n",
    "axs = []\n",
    "\n",
    "for i, year in enumerate(titles):\n",
    "    fig, ax = plt.subplots(figsize=(7, 7)) # 5, 4\n",
    "    # fig.subplots_adjust(top=0.85, bottom=0.15, left=0.2, right=0.5, hspace=0.8)\n",
    "    img = mpimg.imread(f'fig_{year}.png')\n",
    "    imgplot = plt.imshow(img, interpolation='none', vmin=0, vmax=1, aspect='equal')\n",
    "    ax = plt.gca()\n",
    "    ax.axis('off')\n",
    "    ax.grid(b=None)\n",
    "    # fig.patch.set_linewidth(0.1)\n",
    "    # fig.patch.set_edgecolor('black')  \n",
    "    plt.title(\"$\\it{T}$$_%s$ (%s): $\\it{n}$=%s, $\\it{E}$=%s, $\\it{d}$=%s\" % (i, year, titles[year][0], order[year], size[year]), fontdict = {'fontsize' : 9})\n",
    "    fig = ax.get_figure()\n",
    "    fig.set_facecolor(\"w\")\n",
    "    fig.savefig(f\"graph_{year}_large.png\", bbox_inches=\"tight\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and load data from text + users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../data/full\"\n",
    "# columns = [\"id_str\", \"user.screen_name\", \"retweeted_status.user.screen_name\", \"retweeted_status.full_text\", \"retweeted_status.id_str\", \"user.verified\", \"retweeted_status.user.verified\"]\n",
    "# new_columns = [\"id\", \"source\", \"target\", \"text\", \"rt_id\", \"source_verified\", \"target_verified\"]\n",
    "\n",
    "# for x in reversed(sorted(os.listdir(path))):\n",
    "#     if x.startswith(\"retweets_\"):\n",
    "#         year = x.split(\"_\")[1].split(\"-\")[0]\n",
    "#         with open(f\"{path}/{x}\", \"r\") as f:\n",
    "#             df = pd.DataFrame([json.loads(x) for x in f.readlines()])\n",
    "#             df[\"user.screen_name\"]                  = df[\"user\"].apply(lambda x: x[\"screen_name\"])\n",
    "#             df[\"user.verified\"]                     = df[\"user\"].apply(lambda x: x[\"verified\"])\n",
    "\n",
    "#             df[\"retweeted_status.id_str\"]           = df[\"retweeted_status\"].apply(lambda x: x[\"id_str\"])\n",
    "#             df[\"retweeted_status.full_text\"]        = df[\"retweeted_status\"].apply(lambda x: x[\"full_text\"])\n",
    "#             df[\"retweeted_status.user.screen_name\"] = df[\"retweeted_status\"].apply(lambda x: x[\"user\"][\"screen_name\"])\n",
    "#             df[\"retweeted_status.user.verified\"]    = df[\"retweeted_status\"].apply(lambda x: x[\"user\"][\"verified\"])\n",
    "\n",
    "#             df = df[columns]\n",
    "#             df.columns = new_columns\n",
    "#             df.to_csv(\"text/text_2013.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or load already extracted text data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for x in os.listdir(\"text\"):\n",
    "    year = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "    dfs[year] = pd.read_csv(x, index_col=\"id\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract tokens from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "for year in dfs:\n",
    "    tokens[year] = dfs[year][\"text\"].astype(str).apply(lambda x: Tokenizer(stop_words=nltk.corpus.stopwords.words('portuguese')).tokenize(x.replace(\"\\n\", \" \")))\n",
    "    tokens[year].apply(lambda x: \" \".join(x)).to_csv(f\"tokens/tokens_{year}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute tf-idf (`term frequency * inverse document frequency`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keywords = [\n",
    "    \"manifestacao\",\n",
    "    \"manifestacoes\",\n",
    "    \"manifestante\",\n",
    "    \"manifestantes\",\n",
    "    \"protesto\",\n",
    "    \"protestos\",\n",
    "]\n",
    "\n",
    "features = {\n",
    "    year: get_features(df[\"tokens\"], tfidf=True).drop(search_keywords)\n",
    "    for year, df in dfs.items()\n",
    "}\n",
    "\n",
    "all_features = reduce(lambda x, y: x.add(y, fill_value=0), features.values())\n",
    "all_features = all_features.divide(all_features.max()).sort_values(ascending=False)\n",
    "\n",
    "features = {\n",
    "    year: df.divide(df.max()).apply(lambda x: None if x == 0 else x).dropna()\n",
    "    for year, df in features.items()\n",
    "}\n",
    "\n",
    "community_features = {\n",
    "    year: {\n",
    "        community:\n",
    "            df.drop([k for k in search_keywords if df.get(k, False)])\n",
    "        for community, df in\n",
    "            get_features(df[\"tokens\"], groupby=df[\"leiden\"], tfidf=True).items()\n",
    "    }\n",
    "    for year, df in dfs.items()\n",
    "}\n",
    "community_features = {\n",
    "    year: {\n",
    "        community:\n",
    "            df.divide(df.max()).apply(lambda x: None if x == 0 else x).dropna()\n",
    "        for community, df in\n",
    "            groups.items()\n",
    "    }\n",
    "    for year, groups in community_features.items()\n",
    "}\n",
    "\n",
    "tf = pd.DataFrame({k: d[\"tokens\"].str.split().explode().value_counts() for k, d in dfs.items()})\n",
    "tf = tf.loc[tf.sum(axis=1).sort_values(ascending=False).index]\n",
    "tf = tf.loc[tf.dropna().sum(axis=1).sort_values(ascending=False).index[:100]]\n",
    "tf[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot years correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100\n",
    "\n",
    "# for Y in range(2013,2019):\n",
    "#     py.iplot(\n",
    "#         go.Figure(\n",
    "#             data=go.Scatter(\n",
    "#                 mode=\"markers\",\n",
    "#                 x=all_features.loc[features[Y].index].values.tolist()[:N],\n",
    "#                 y=features[Y].values.tolist()[:N],\n",
    "#                 text=features[Y].index.tolist()[:N],\n",
    "#             ),\n",
    "#             layout=go.Layout(    \n",
    "#                 title=f\"{Y}\",\n",
    "#             )\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot communities correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 2013\n",
    "# N = 100\n",
    "\n",
    "# for C in range(0,7):\n",
    "#     py.iplot(\n",
    "#         go.Figure(\n",
    "#             data=go.Scatter(\n",
    "#                 mode=\"markers\",\n",
    "#                 x=features[Y].loc[community_features[Y][C].index].values.tolist()[:N],\n",
    "#                 y=community_features[Y][C].values.tolist()[:N],\n",
    "#                 text=community_features[Y][C].index.tolist()[:N],\n",
    "#             ),\n",
    "#             layout=go.Layout(    \n",
    "#                 title=f\"{Y}: Comunidade {C}\",\n",
    "#             )\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets from target nodes (retweeted users) within top Leiden communities (Cn, n<7)\n",
    "\n",
    "Consider only the top 7 communities previously identified by the Leiden algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden = {}\n",
    "for x in os.listdir(\"graphs\"):\n",
    "    if x.startswith(\"centrality_\"):\n",
    "        year = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "        leiden[year] = pd.read_csv(f\"graphs/{x}\", index_col=\"id\")[\"leiden\"]\n",
    "        leiden[year] = leiden[year][leiden[year].isin([0,1,2,3,4,5,6])]\n",
    "\n",
    "for year in dfs:\n",
    "    dfs[year][\"leiden\"] = dfs[year][\"target\"].apply(lambda x: leiden[year].get(x, None))\n",
    "    print(year, \"=>\", dfs[year].shape[0], \"tweets =>\", dfs[year][\"leiden\"].dropna().shape[0], \"within top 7 communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check verified vs. non-verified tweets & users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2013, 2019):\n",
    "    print(\n",
    "        year,\n",
    "        dfs[year].query(\"target_verified == 'True'\")[\"rt_id\"].unique().shape[0],\n",
    "        \"verified vs.\",\n",
    "        dfs[year].query(\"target_verified == 'False'\")[\"rt_id\"].unique().shape[0],\n",
    "        \"non-verified unique tweets\"\n",
    "    )\n",
    "print()\n",
    "for year in range(2013, 2019):\n",
    "    print(\n",
    "        year,\n",
    "        dfs[year].query(\"target_verified == 'True'\")[\"target\"].unique().shape[0],\n",
    "        \"verified vs.\",\n",
    "        dfs[year].query(\"target_verified == 'False'\")[\"target\"].unique().shape[0],\n",
    "        \"non-verified unique users\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ...same thing, now per community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs.values())\n",
    "\n",
    "for n in range(7):\n",
    "    print(\n",
    "        f\"C{n+1}\",\n",
    "        df[df[\"leiden\"]==n].query(\"target_verified == 'True'\")[\"rt_id\"].unique().shape[0],\n",
    "        \"verified vs.\",\n",
    "        df[df[\"leiden\"]==n].query(\"target_verified == 'False'\")[\"rt_id\"].unique().shape[0],\n",
    "        \"non-verified unique tweets\"\n",
    "    )\n",
    "print()\n",
    "for n in range(7):\n",
    "    print(\n",
    "        f\"C{n+1}\",\n",
    "        df[df[\"leiden\"]==n].query(\"target_verified == 'True'\")[\"target\"].unique().shape[0],\n",
    "        \"verified vs.\",\n",
    "        df[df[\"leiden\"]==n].query(\"target_verified == 'False'\")[\"target\"].unique().shape[0],\n",
    "        \"non-verified unique users\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and hyperlinks (unshortened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets and links published from top communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p retweets urls\n",
    "\n",
    "retweets = {}\n",
    "leiden_rts = {}\n",
    "\n",
    "for x in range(2013, 2019):\n",
    "    retweets[x] = pd.read_csv(f\"../retweets/retweets_{x}.csv\", dtype=str)\n",
    "    retweets[x][\"leiden\"] = retweets[x][\"leiden\"].apply(lambda x: int(float(x)) if type(x) == str else x)\n",
    "\n",
    "for c in range(7):\n",
    "    leiden_rts[c] = pd.concat([_[_[\"leiden\"] == c] for _ in retweets.values()])\n",
    "retweets = pd.concat([_ for _ in retweets.values()])\n",
    "\n",
    "# links = {}\n",
    "# links_ = {}\n",
    "# for c in range(7):\n",
    "#     links[c] = {}\n",
    "#     links_[c] = pd.concat([_[_[\"leiden\"] == c] for _ in urls.values()])[\"full_url\"].explode().value_counts().to_dict()\n",
    "#     for y in [2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "#         links[c][y] = urls[y][urls[y][\"leiden\"] == c][\"full_url\"].explode().value_counts().to_dict()\n",
    "\n",
    "# with open(\"../json/links.json\", \"w\") as j:\n",
    "#     json.dump(links, j)\n",
    "with open(\"../json/links.json\", \"r\") as j:\n",
    "    links = {int(k): v for k, v in json.load(j).items()}\n",
    "\n",
    "# with open(\"../json/links_.json\", \"w\") as j:\n",
    "#     json.dump(links_, j)\n",
    "with open(\"../json/links_.json\", \"r\") as j:\n",
    "    links_ = {int(k): v for k, v in json.load(j).items()}\n",
    "\n",
    "urls = pd.read_csv(\"../urls/urls_UNSHORTENED.tab\", delimiter=\"\\t\", index_col=\"url\", usecols=[\"full_url\", \"url\"], squeeze=True).explode()\n",
    "urls = {v: k for k, v in urls.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs published from top communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"../urls/urls_UNSHORTENED.tab\", index_col=\"url\", delimiter=\"\\t\")\n",
    "dct = d[d[\"is_short\"] == True][\"full_url\"].dropna().apply(lambda x: json.loads(x) if type(x) == str and not x.startswith(\"http\") else x).to_dict()\n",
    "\n",
    "for y in [2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    leiden[y] = gk.nodes(gk.graph(f\"graphs/graph_{y}.gexf\"))[\"leiden\"]\n",
    "    urls[y] = pd.read_csv(f\"urls/urls_{y}.csv\")\n",
    "    urls[y][\"leiden\"] = urls[y][\"from_user\"].apply(lambda x: leiden[y][x])\n",
    "    urls[y][\"url\"] = urls[y][\"urls\"].apply(lambda x: x.replace(\"https:\", \"http:\").lstrip('\"').rstrip('\"'))\n",
    "    urls[y][\"full_url\"] = urls[y][\"url\"].apply(lambda x: dct.get(x, x) if d.get(x, x) != \"\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ...after loading the data, the analysis then may be done by usnig the produced dictionary of unshortened links per community: `links`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text analysis with [Scattertext](https://github.com/JasonKessler/scattertext) (@jasonkessler)\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations. 2017.\n",
    "\n",
    "> https://arxiv.org/abs/1703.00565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load portuguese spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optionally use Stanza (2020) - requires some code adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza\n",
    "# #stanza.download(\"pt\")\n",
    "# nlp = stanza.Pipeline('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data sets\n",
    "\n",
    "Let's first grab the 2012 political convention data set used as example in Scattertext and preview it, then replicate the structure with our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = st.SampleCorpora.ConventionData2012.get_data()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = {}\n",
    "# for x in sorted(os.listdir(\".\")):\n",
    "#     if x.startswith(\"scatter_2\"):\n",
    "#         year = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "#         dfs[year] = pd.read_csv(f\"{x}\", dtype=str, index_col=\"id\").astype(str)\n",
    "#         dfs[year][\"leiden\"] = dfs[year][\"leiden\"].apply(lambda x: f\"C{int(float(x))+1}\" if x != \"nan\" else \"Cn\")\n",
    "#         dfs[year] = dfs[year][[\"leiden\", \"source\", \"text\"]] # \"text\"\n",
    "#         dfs[year].columns = [\"party\", \"speaker\", \"text\"]\n",
    "# df_source = pd.concat([_ for _ in dfs.values()]).drop_duplicates()\n",
    "\n",
    "# dfs = {}\n",
    "# for x in sorted(os.listdir(\".\")):\n",
    "#     if x.startswith(\"scatter_2\"):\n",
    "#         year = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "#         dfs[year] = pd.read_csv(f\"{x}\", dtype=str, index_col=\"id\").astype(str)\n",
    "#         dfs[year][\"leiden\"] = dfs[year][\"leiden\"].apply(lambda x: f\"C{int(float(x))+1}\" if x != \"nan\" else \"Cn\")\n",
    "#         dfs[year] = dfs[year][[\"leiden\", \"target\", \"text\"]] # \"text\"\n",
    "#         dfs[year].columns = [\"party\", \"speaker\", \"text\"]\n",
    "# df_target = pd.concat([_ for _ in dfs.values()]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview stats from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_source\n",
    "# print(\"Document Count\")\n",
    "# print(df.groupby('party')['text'].count())\n",
    "# print(\"Word Count\")\n",
    "# print(df.groupby('party').apply(lambda x: x.text.apply(lambda x: len(x.split())).sum()))\n",
    "# print(\"total:\", df.text.apply(lambda x: len(x.split())).sum())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_target\n",
    "# print(\"Document Count\")\n",
    "# print(df.groupby('party')['text'].count())\n",
    "# print(\"Word Count\")\n",
    "# print(df.groupby('party').apply(lambda x: x.text.apply(lambda x: len(x.split())).sum()))\n",
    "# print(\"total:\", df.text.apply(lambda x: len(x.split())).sum())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse full corpus from data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using target node information (retweeted users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time df_target[\"parsed\"] = df_target.text.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time corpus_t = st.CorpusFromParsedDocuments(df_target, category_col='party', parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"corpus/corpus_target.pickle\", \"wb\") as p:\n",
    "#     pickle.dump(corpus_t, p, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using source node information (retweeting users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time df_source[\"parsed\"] = df_source.text.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time corpus_s = st.CorpusFromParsedDocuments(df_source, category_col='party', parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"corpus/corpus_source.pickle\", \"wb\") as p:\n",
    "#     pickle.dump(corpus_s, p, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse data considering time intervals (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preview stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for year in range(2013,2019):\n",
    "#     print(year)\n",
    "#     df = dfs[year]\n",
    "#     print(\"Document Count\")\n",
    "#     print(df.groupby('party')['text'].count())\n",
    "#     print(\"Word Count\")\n",
    "#     print(df.groupby('party').apply(lambda x: x.text.apply(lambda x: len(x.split())).sum()))\n",
    "#     print(\"total:\", df.text.apply(lambda x: len(x.split())).sum())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn into Scattertext corpora and parse with spaCy (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpora = {}\n",
    "# for year, df in dfs.items():\n",
    "#     df[\"parsed\"] = df.text.apply(nlp)\n",
    "#     corpora[year] = st.CorpusFromParsedDocuments(df, category_col='party', parsed_col='parsed').build()\n",
    "#     with open(f\"corpora/corpus_{year}.pickle\", \"wb\") as p:\n",
    "#         pickle.dump(corpus, p, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load full corpus from parsed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time with open(\"../corpus/corpus_target.pickle\", \"rb\") as p:\\\n",
    "#     corpus = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Loaded corpus with:\\n\\\n",
    "# {corpus.get_num_categories()} categories\\n\\\n",
    "# {corpus.get_num_docs()} documents\\n\\\n",
    "# {corpus.get_num_terms()} terms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternatively consider lemmatized words\n",
    "\n",
    "> e.g. (`vem pra rua` => `ir pra rua`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../corpus/corpus_target.pickle\", \"rb\") as p:\n",
    "#     %time corpus = st.CorpusFromParsedDocuments(\\\n",
    "#         pickle.load(p).get_df(),\\\n",
    "#         category_col='party',\\\n",
    "#         parsed_col='parsed',\\\n",
    "#         feats_from_spacy_doc=st.FeatsFromSpacyDoc(use_lemmas=True),\\\n",
    "#     ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Loaded corpus with:\\n\\\n",
    "# {corpus.get_num_categories()} categories\\n\\\n",
    "# {corpus.get_num_docs()} documents\\n\\\n",
    "# {corpus.get_num_terms()} terms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Lemmatization resulted in {1370600 - 1183422} less n-grams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternatively consider bigrams only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = corpus.remove_terms([x for x in corpus.get_terms() if \" \" not in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Filtering for bigrams only resulted in {1183422 - 926533} removed unigrams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select stopwords to remove from corpus and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_terms = [\"protesto\", \"protestos\", \"vemprarua\", \"manifestação\", \"manifestações\", \"manifestante\", \"manifestantes\", \"manifestacao\", \"manifestacoes\"]\n",
    "# extra_stopwords = [\"ai\",\"av\",\"ces\",\"ir\",\"ne\",\"pra\",\"pros\",\"pq\",\"rt\",\"ta\",\"tao\",\"to\",\"vai\",\"vc\",\"vcs\",\"sent\",\"zs\",\"zc\",\"ne\",\"rs\",\"ver\"]\n",
    "# nltk_stopwords = list(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "# allow_stopwords = ['ela','elas','ele','eles','não']\n",
    "# consider_stopwords = set([x for x in nltk_stopwords+extra_stopwords])\n",
    "# [consider_stopwords.remove(x) for x in allow_stopwords]\n",
    "# remove_terms = [x for x in corpus.get_terms() if not is_clean(x, consider_stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before = corpus.get_num_terms()\n",
    "# corpus = corpus.remove_terms(remove_terms)\n",
    "# print(f\"Removed {before-corpus.get_num_terms()} terms considering the {len(consider_stopwords)} stopwords selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_num_docs = int(corpus.get_num_docs()/10000)+1\n",
    "# print(f\"Removing n-grams observed in >{min_num_docs} docs (0.01%).\")\n",
    "# corpus = corpus.remove_terms_used_in_less_than_num_docs(min_num_docs)\n",
    "# print(\"Finished cleaning, resulted in %s n-grams (n = {1,2}).\" % corpus.get_num_terms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.get_term_count_df().sort_values(\"corpus\", ascending=False).to_csv(\"term_count.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Save cleaned corpus as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/media/dislocker/nvme0n1p5/corpus_cleaned.pickle\", \"wb\") as p:\n",
    "#     %time pickle.dump(corpus, p, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate scattertext plots from cleaned corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/dislocker/nvme0n1p5/corpus_cleaned.pickle\", \"rb\") as p:\n",
    "    corpus = pickle.load(p) # protocol=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_range = list(range(1,8))\n",
    "num_terms = 2000\n",
    "# corpus = corpus.get_unigram_corpus()\n",
    "# corpus = corpus.compact(st.AssociationCompactor(num_terms))\n",
    "\n",
    "for cat in cat_range:\n",
    "    print(cat)\n",
    "#     if cat != 1: continue\n",
    "    html = st.produce_scattertext_explorer(\n",
    "        corpus.compact(st.AssociationCompactor(num_terms)),\n",
    "        category=f'C{cat}',\n",
    "        category_name=f'Comunidade {cat} (C{cat})',\n",
    "        metadata=corpus.compact(st.AssociationCompactor(num_terms)).get_df()['speaker'],\n",
    "        not_category_name='Outras comunidades (Cn: {n ∈ ℕ | 0 < n < 8; n≠%s})'%cat,\n",
    "        not_categories=[f\"C{x}\" for x in cat_range if x != cat],\n",
    "        minimum_term_frequency=0,\n",
    "        pmi_threshold_coefficient=0,\n",
    "        width_in_pixels=700,\n",
    "        height_in_pixels=400,\n",
    "        max_overlapping=1,\n",
    "        scores=corpus.compact(st.AssociationCompactor(num_terms)).get_scaled_f_scores(f'C{cat}', beta=0.5),\n",
    "#         scores=corpus.compact(st.AssociationCompactor(num_terms)).get_scaled_f_scores(f'C{cat}', beta=0.5),\n",
    "        transform=st.Scalers.dense_rank,\n",
    "        sort_by_dist=False,\n",
    "#         max_terms=200,\n",
    "    #     transform=st.Scalers.scale,\n",
    "#         transform=st.Scalers.percentile, # also try with \"jitter\"\n",
    "    #     jitter=0.1,\n",
    "#         transform=st.Scalers.log_scale_standardize,\n",
    "    #     term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n",
    "    #     x_coords=frequencies_scaled,\n",
    "    #     y_coords=corpus.get_scaled_f_scores('democrat', beta=0.5),\n",
    "    #     x_label='Log Frequency',\n",
    "    #     y_label='Scaled F-Score')\n",
    "    #     sort_by_dist=False,\n",
    "        top_terms_length=21,\n",
    "    )\n",
    "    file_name = f'scatter/scatter_cat_{cat}.html'\n",
    "    with open(f'scatter/scatter_cat_{cat}.html', 'wb') as f:\n",
    "        f.write(html.encode('utf-8'))\n",
    "    # IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmaps = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3e86f05d8baddf70cf80e0364559f9b03ed7f30f85373a81ae132884045b90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
